{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bef23f70-ca5f-47f0-9c29-d002284c0e9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# ==============================================================================\n",
    "# Constants & Configuration\n",
    "# ==============================================================================\n",
    "# Mapeamento explícito: { \"Nome Original Sujo\": \"nome_novo_limpo\" }\n",
    "COLUMN_MAPPING = {\n",
    "    \"Year\": \"year\",\n",
    "    \"Continent\": \"continent\",\n",
    "    \"Country\": \"country\",\n",
    "    \"Avg_Temperature(°C)\": \"avg_temperature_celsius\",\n",
    "    \"CO2_Emissions(Mt)\": \"co2_emissions_mt\",\n",
    "    \"Sea_Level_Rise(mm)\": \"sea_level_rise_mm\",\n",
    "    \"Climate_Risk_Index\": \"climate_risk_index\"\n",
    "}\n",
    "\n",
    "TABLE_NAME_SILVER = \"silver_climate_data\"\n",
    "\n",
    "# ==============================================================================\n",
    "# Transformation Functions\n",
    "# ==============================================================================\n",
    "\n",
    "def rename_columns(df: DataFrame, mapping: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Renames columns based on a dictionary mapping to ensure snake_case standardization.\n",
    "    \"\"\"\n",
    "    for old_name, new_name in mapping.items():\n",
    "        df = df.withColumnRenamed(old_name, new_name)\n",
    "    return df\n",
    "\n",
    "def check_data_quality(df: DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Performs basic quality checks: Null counts and Duplicate checks.\n",
    "    Prints the report to stdout.\n",
    "    \"\"\"\n",
    "    print(\"--- Data Quality Report ---\")\n",
    "    \n",
    "    # 1. Check for Nulls\n",
    "    # Cria uma expressão dinâmica para contar nulos em todas as colunas\n",
    "    null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    print(\"\\nNull Values per Column:\")\n",
    "    null_counts.show()\n",
    "    \n",
    "    # 2. Check for Duplicates\n",
    "    total_count = df.count()\n",
    "    distinct_count = df.distinct().count()\n",
    "    duplicate_count = total_count - distinct_count\n",
    "    \n",
    "    print(f\"Total Rows: {total_count}\")\n",
    "    print(f\"Duplicate Rows: {duplicate_count}\")\n",
    "    \n",
    "    if duplicate_count > 0:\n",
    "        print(\"⚠️ Warning: Duplicates detected.\")\n",
    "    else:\n",
    "        print(\"✅ Quality Check: No duplicates found.\")\n",
    "\n",
    "def save_as_delta_table(df: DataFrame, table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the DataFrame as a managed Delta Table in the Hive Metastore.\n",
    "    Mode 'overwrite' ensures idempotency (can run multiple times without duplicating data).\n",
    "    \"\"\"\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    print(f\"\\n✅ Success: Data saved to table '{table_name}'\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Execution\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Load Bronze Data (Já carregado no notebook anterior ou via referência direta)\n",
    "# Para garantir que temos o DF, vamos reler rapidinho do volume (boas práticas de isolamento de notebook)\n",
    "file_path = \"/Volumes/workspace/default/mvp_engenharia/global_climate_change_2020_2025.csv\"\n",
    "df_bronze = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_path)\n",
    "\n",
    "# 2. Transform (Rename)\n",
    "df_silver = rename_columns(df_bronze, COLUMN_MAPPING)\n",
    "\n",
    "# 3. Quality Assurance\n",
    "check_data_quality(df_silver)\n",
    "\n",
    "# 4. Load (Save to Silver)\n",
    "save_as_delta_table(df_silver, TABLE_NAME_SILVER)\n",
    "\n",
    "# 5. Final Verification\n",
    "display(spark.sql(f\"SELECT * FROM {TABLE_NAME_SILVER} LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49851d82-382e-43e2-970c-07c3d98adb8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gera estatísticas descritivas para preencher o Catálogo de Dados\n",
    "display(df_silver.describe())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_transformacao_prata",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}